{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "PRIqV4rL7weF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support, f1_score, recall_score, precision_score, accuracy_score\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, RUSBoostClassifier\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# import re\n",
        "# import unicodedata\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOQYbjnM70tK",
        "outputId": "dccdea25-6d8a-4b38-ab01-22a1adeb9e65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting data"
      ],
      "metadata": {
        "id": "8UoQ74U-7-Cq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0RRbSwZO7Drw"
      },
      "outputs": [],
      "source": [
        "def get_data():\n",
        "    root = \"./drive/MyDrive/inf554/\"\n",
        "    path_to_training = Path(root + \"training\")\n",
        "    path_to_test = Path(root + \"test\")\n",
        "\n",
        "    def flatten(list_of_list):\n",
        "        return [item for sublist in list_of_list for item in sublist]\n",
        "    #####\n",
        "    # training and test sets of transcription ids\n",
        "    #####\n",
        "    training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
        "    training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
        "    training_set.remove('IS1002a')\n",
        "    training_set.remove('IS1005d')\n",
        "    training_set.remove('TS3012c')\n",
        "\n",
        "    test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
        "    test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])\n",
        "\n",
        "\n",
        "    #####\n",
        "    # text_baseline: utterances are embedded with SentenceTransformer, then train a classifier.\n",
        "    #####\n",
        "\n",
        "    y_training = []\n",
        "    graph = []\n",
        "    last_graph_size = 0\n",
        "    with open(root + \"training_labels.json\", \"r\") as file:\n",
        "        training_labels = json.load(file)\n",
        "\n",
        "    X_training_original = []\n",
        "    for transcription_id in tqdm(training_set):\n",
        "        with open(path_to_training / f\"{transcription_id}.json\", \"r\") as file:\n",
        "            transcription = json.load(file)\n",
        "\n",
        "        with open(path_to_training / f\"{transcription_id}.txt\", 'r') as file:\n",
        "            relations = file.readlines()\n",
        "\n",
        "        graph.extend([(lambda x: (int(x[0]) + last_graph_size, int(x[2]) + last_graph_size, x[1])) (rel.split(' ')) for rel in relations])\n",
        "\n",
        "        last_graph_size += len(transcription)\n",
        "\n",
        "\n",
        "        for utterance in transcription:\n",
        "            X_training_original.append(utterance[\"speaker\"] + \": \" + utterance[\"text\"])\n",
        "\n",
        "        y_training += training_labels[transcription_id]\n",
        "    return X_training_original, y_training, graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "CDxheVxG8Ca8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, rel = get_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUPTa-to8E4f",
        "outputId": "449bb656-07f5-47c6-cc98-c121040ee42d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [00:00<00:00, 133.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, labels = X, y"
      ],
      "metadata": {
        "id": "4NbeQu4e89Fr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = {e:i for i, e in enumerate(set(e[2] for e in rel))}; categories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_HdrP7s8Jik",
        "outputId": "9e6a25fa-4544-4c8d-d7b4-8e1a8ef062b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Explanation': 0,\n",
              " 'Narration': 1,\n",
              " 'Contrast': 2,\n",
              " 'Continuation': 3,\n",
              " 'Q-Elab': 4,\n",
              " 'Background': 5,\n",
              " 'Correction': 6,\n",
              " 'Elaboration': 7,\n",
              " 'Comment': 8,\n",
              " 'Acknowledgement': 9,\n",
              " 'Conditional': 10,\n",
              " 'Question-answer_pair': 11,\n",
              " 'Parallel': 12,\n",
              " 'Result': 13,\n",
              " 'Alternation': 14,\n",
              " 'Clarification_question': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "daSVUtGl8nKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi73wfEw8Vwj",
        "outputId": "69d65281-6c62-4878-a3ba-28dc5eecbd58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "dgDHni8J8rij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Convert labels to tensor\n",
        "labels = torch.tensor(labels)\n"
      ],
      "metadata": {
        "id": "in99w1dn8wFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcjgzZOX-Jtv",
        "outputId": "975f48e6-87c0-41d1-cb0e-1ab31cc8f0b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  7610,  1024,  ...,     0,     0,     0],\n",
              "        [  101,  7610,  1024,  ...,     0,     0,     0],\n",
              "        [  101,  7610,  1024,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101,  2033,  1024,  ...,     0,     0,     0],\n",
              "        [  101, 21318,  1024,  ...,     0,     0,     0],\n",
              "        [  101,  2033,  1024,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Define a custom dataset\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create the dataset\n",
        "dataset = SentenceDataset(inputs, labels.tolist())\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "RPqnw1Sc-Eh-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in trange(10):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the training loss\n",
        "    print(f'Epoch: {epoch+1}, Training Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFaUMWvK9DaH",
        "outputId": "0690e624-0b63-43ec-bf6d-797e66589708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "val_preds = []\n",
        "val_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate F1-score for the validation set\n",
        "val_f1 = f1_score(val_labels, val_preds)\n",
        "print(f'Validation F1 Score: {val_f1}')\n"
      ],
      "metadata": {
        "id": "wOhp-KBE_1L0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}