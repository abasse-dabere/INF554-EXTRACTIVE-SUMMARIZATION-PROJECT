{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2021/abasse.dabere/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /users/eleves-a/2021/abasse.dabere/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training = Path(\"training\")\n",
    "path_to_test = Path(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinaison Transcription+graphe pour chaque dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]*>|(?:um|uh)', '', text)\n",
    "\n",
    "    # Tokenization des mots\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Supprimer les mots vides (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stemming (réduction à la racine des mots)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Rejoindre les mots traités en une seule chaîne de texte\n",
    "    processed_text = ' '.join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def get_xi(transcription_id):\n",
    "       discourse_graph = [] # list, i attribute j\n",
    "       attributes = dict() #dict, i -> \"attribute1 attribute2\"\n",
    "       # transcription = [] # list, idx -> dict(\"speaker\", \"text\", \"index\")\n",
    "       x_i = [] # list, speaker: attribute: text\n",
    "       attr_i = [] # list, attributes\n",
    "\n",
    "       with open(path_to_training / f\"{transcription_id}.json\", 'r') as f:\n",
    "              transcription = json.load(f)\n",
    "\n",
    "       with open(path_to_training / f\"{transcription_id}.txt\", 'r') as f:\n",
    "              for line in f: discourse_graph.append(line.strip())\n",
    "\n",
    "       for line in discourse_graph:\n",
    "              tmp = line.split()\n",
    "              idx = int(tmp[-1])\n",
    "              attributes[idx] = attributes.get(idx, \"\")+ tmp[1]\n",
    "\n",
    "       for i in range(len(transcription)):\n",
    "              replique = transcription[i]\n",
    "              text = preprocess_text(replique['text'])\n",
    "              attr_i.append(attributes.get(i,\"\"))\n",
    "              x_i.append(text)\n",
    "       \n",
    "       return x_i, attr_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenater des x_i -> X et concatenation des y_i -> y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recup des ids\n",
    "transcription_ids = []\n",
    "\n",
    "transcripts = path_to_training.glob('*.json')\n",
    "for transcript in transcripts:\n",
    "    transcription_ids.append(transcript.name[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation de X contenant les repliques x_i\n",
    "X = [] #list des repliques\n",
    "A = [] #list des attributs\n",
    "for transcription_id in  transcription_ids:\n",
    "    x_i, attr_i = get_xi(transcription_id)\n",
    "    X.extend(x_i)\n",
    "    A.extend(attr_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72623\n",
      "72623\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72623, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.concatenate([np.array(X).reshape(-1,1), np.array(A).reshape(-1,1)], axis=1)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation de y contenant les labels pour chaque x_i\n",
    "y = [] # concatenation des labels\n",
    "with open(\"training_labels.json\", 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "for transcription_id in transcription_ids:\n",
    "    y.extend(labels[transcription_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séperation en Train et Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train, Z_valid, y_train, y_valid = train_test_split(Z, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train = Z_train[:,1]\n",
    "X_train = Z_train[:,0]\n",
    "\n",
    "A_valid = Z_valid[:,1]\n",
    "X_valid = Z_valid[:,0]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2021/abasse.dabere/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1589/1589 [00:09<00:00, 169.75it/s]\n",
      "Batches: 100%|██████████| 681/681 [00:03<00:00, 179.56it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_bert = bert.encode(X_train, show_progress_bar=True)\n",
    "X_valid_bert = bert.encode(X_valid, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train_df = pd.Series(A_train)\n",
    "A_valid_df = pd.Series(A_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = list(A_train_df.unique())\n",
    "dic = dict(zip(attributes, [str(i) for i in range(len(attributes))]))\n",
    "\n",
    "def b_dummies(x):\n",
    "    return dic.get(x, '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_col = [str(i) for i in range(-1,len(attributes))] # with -1\n",
    "\n",
    "A_train_df = A_train_df.apply(b_dummies)\n",
    "A_train_df = pd.get_dummies(A_train_df, dtype=float)\n",
    "A_train_df = A_train_df.reindex(columns=ordered_col, fill_value=0)\n",
    "A_train = A_train_df.values\n",
    "\n",
    "A_valid_df = A_valid_df.apply(b_dummies)\n",
    "A_valid_df = pd.get_dummies(A_valid_df, dtype=float)\n",
    "A_valid_df = A_valid_df.reindex(columns=ordered_col, fill_value=0)\n",
    "A_valid = A_valid_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train = np.concatenate([X_train_bert, A_train], axis=1)\n",
    "Z_valid = np.concatenate([X_valid_bert, A_valid], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele avec DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21787, 402)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 18:37:09.812635: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.838992: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.839200: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.840627: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.840794: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.840935: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.841185: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.841334: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.841497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 18:37:09.841612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18466 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Dense(256, input_dim=402, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 18:37:12.899633: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f2bac9552e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-25 18:37:12.899662: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-11-25 18:37:12.943602: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-25 18:37:13.027145: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1700933833.119790 1667159 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589/1589 [==============================] - 7s 2ms/step - loss: 0.3528 - accuracy: 0.8257 - val_loss: 0.3294 - val_accuracy: 0.8377\n",
      "Epoch 2/20\n",
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.3388 - accuracy: 0.8336 - val_loss: 0.3246 - val_accuracy: 0.8401\n",
      "Epoch 3/20\n",
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.3329 - accuracy: 0.8363 - val_loss: 0.3238 - val_accuracy: 0.8387\n",
      "Epoch 4/20\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.3271 - accuracy: 0.8389 - val_loss: 0.3235 - val_accuracy: 0.8409\n",
      "Epoch 5/20\n",
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.3209 - accuracy: 0.8436 - val_loss: 0.3217 - val_accuracy: 0.8422\n",
      "Epoch 6/20\n",
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.3159 - accuracy: 0.8471 - val_loss: 0.3232 - val_accuracy: 0.8366\n",
      "Epoch 7/20\n",
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.3090 - accuracy: 0.8489 - val_loss: 0.3253 - val_accuracy: 0.8374\n",
      "Epoch 8/20\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.3027 - accuracy: 0.8559 - val_loss: 0.3247 - val_accuracy: 0.8364\n",
      "Epoch 9/20\n",
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.2951 - accuracy: 0.8588 - val_loss: 0.3261 - val_accuracy: 0.8377\n",
      "Epoch 10/20\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.2864 - accuracy: 0.8657 - val_loss: 0.3349 - val_accuracy: 0.8327\n",
      "Epoch 11/20\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.2792 - accuracy: 0.8701 - val_loss: 0.3379 - val_accuracy: 0.8323\n",
      "Epoch 12/20\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.2735 - accuracy: 0.8743 - val_loss: 0.3455 - val_accuracy: 0.8344\n",
      "Epoch 13/20\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.2648 - accuracy: 0.8789 - val_loss: 0.3480 - val_accuracy: 0.8333\n",
      "Epoch 14/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2597 - accuracy: 0.8822 - val_loss: 0.3458 - val_accuracy: 0.8276\n",
      "Epoch 15/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2538 - accuracy: 0.8860 - val_loss: 0.3489 - val_accuracy: 0.8325\n",
      "Epoch 16/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2474 - accuracy: 0.8888 - val_loss: 0.3544 - val_accuracy: 0.8305\n",
      "Epoch 17/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2408 - accuracy: 0.8931 - val_loss: 0.3573 - val_accuracy: 0.8298\n",
      "Epoch 18/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2341 - accuracy: 0.8966 - val_loss: 0.3722 - val_accuracy: 0.8292\n",
      "Epoch 19/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2302 - accuracy: 0.8992 - val_loss: 0.3638 - val_accuracy: 0.8253\n",
      "Epoch 20/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2251 - accuracy: 0.9016 - val_loss: 0.3914 - val_accuracy: 0.8312\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_history = model.fit(Z_train, y_train, epochs=20, batch_size=32, validation_data=(Z_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/681 [..............................] - ETA: 46s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681/681 [==============================] - 1s 852us/step\n",
      "0.5025030442429982\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(Z_valid)\n",
    "y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "\n",
    "print(f1_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele avec DNN avec class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, input_dim=402, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_0 = len(y_train) - np.sum(y_train)\n",
    "count_class_1 = np.sum(y_train)\n",
    "\n",
    "total = len(y_train)\n",
    "\n",
    "frequency_class_0 = count_class_0 / total\n",
    "frequency_class_1 = count_class_1 / total\n",
    "\n",
    "inverse_weight_class_0 = 1 / frequency_class_0\n",
    "inverse_weight_class_1 = 1 / frequency_class_1\n",
    "\n",
    "class_weights = {0: inverse_weight_class_0, 1: inverse_weight_class_1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1589/1589 [==============================] - 5s 3ms/step - loss: 0.9527 - accuracy: 0.7217 - val_loss: 0.4569 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.8954 - accuracy: 0.7384 - val_loss: 0.4347 - val_accuracy: 0.7475\n",
      "Epoch 3/10\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.8813 - accuracy: 0.7481 - val_loss: 0.4555 - val_accuracy: 0.7471\n",
      "Epoch 4/10\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.8599 - accuracy: 0.7573 - val_loss: 0.4328 - val_accuracy: 0.7692\n",
      "Epoch 5/10\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.8428 - accuracy: 0.7660 - val_loss: 0.4358 - val_accuracy: 0.7623\n",
      "Epoch 6/10\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.8227 - accuracy: 0.7767 - val_loss: 0.4365 - val_accuracy: 0.7666\n",
      "Epoch 7/10\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.8053 - accuracy: 0.7836 - val_loss: 0.4441 - val_accuracy: 0.7612\n",
      "Epoch 8/10\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.7869 - accuracy: 0.7895 - val_loss: 0.4345 - val_accuracy: 0.7728\n",
      "Epoch 9/10\n",
      "1589/1589 [==============================] - 4s 3ms/step - loss: 0.7687 - accuracy: 0.7982 - val_loss: 0.4405 - val_accuracy: 0.7727\n",
      "Epoch 10/10\n",
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.7535 - accuracy: 0.7994 - val_loss: 0.4304 - val_accuracy: 0.7788\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_history = model.fit(Z_train, y_train, epochs=10, batch_size=32, validation_data=(Z_valid, y_valid), class_weight= class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681/681 [==============================] - 1s 885us/step\n",
      "0.5652958152958153\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(Z_valid)\n",
    "y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "\n",
    "print(f1_score(y_valid, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
