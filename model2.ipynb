{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2021/abasse.dabere/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /users/eleves-a/2021/abasse.dabere/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2023-11-23 16:32:49.773722: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-23 16:32:49.773763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-23 16:32:50.006350: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-23 16:32:50.466225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 16:32:52.784046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training = Path(\"training\")\n",
    "path_to_test = Path(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinaison Transcription+graphe pour chaque dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]*>|(?:um|uh)', '', text)\n",
    "\n",
    "    # Tokenization des mots\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Supprimer les mots vides (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stemming (réduction à la racine des mots)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Rejoindre les mots traités en une seule chaîne de texte\n",
    "    processed_text = ' '.join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def get_xi(transcription_id):\n",
    "       discourse_graph = [] # list, i attribute j\n",
    "       attributes = dict() #dict, i -> \"attribute1 attribute2\"\n",
    "       # transcription = [] # list, idx -> dict(\"speaker\", \"text\", \"index\")\n",
    "       x_i = [] # list, speaker: attribute: text\n",
    "       attr_i = [] # list, attributes\n",
    "\n",
    "       with open(path_to_training / f\"{transcription_id}.json\", 'r') as f:\n",
    "              transcription = json.load(f)\n",
    "\n",
    "       with open(path_to_training / f\"{transcription_id}.txt\", 'r') as f:\n",
    "              for line in f: discourse_graph.append(line.strip())\n",
    "\n",
    "       for line in discourse_graph:\n",
    "              tmp = line.split()\n",
    "              idx = int(tmp[-1])\n",
    "              attributes[idx] = attributes.get(idx, \"\")+ tmp[1]\n",
    "\n",
    "       for i in range(len(transcription)):\n",
    "              replique = transcription[i]\n",
    "              text = preprocess_text(replique['text'])\n",
    "              attr_i.append(attributes.get(i,\"\"))\n",
    "              x_i.append(text)\n",
    "       \n",
    "       return x_i, attr_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenater des x_i -> X et concatenation des y_i -> y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recup des ids\n",
    "transcription_ids = []\n",
    "\n",
    "transcripts = path_to_training.glob('*.json')\n",
    "for transcript in transcripts:\n",
    "    transcription_ids.append(transcript.name[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation de X contenant les repliques x_i\n",
    "X = [] #list des repliques\n",
    "A = [] #list des attributs\n",
    "for transcription_id in  transcription_ids:\n",
    "    x_i, attr_i = get_xi(transcription_id)\n",
    "    X.extend(x_i)\n",
    "    A.extend(attr_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72623\n",
      "72623\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72623, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.concatenate([np.array(X).reshape(-1,1), np.array(A).reshape(-1,1)], axis=1)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation de y contenant les labels pour chaque x_i\n",
    "y = [] # concatenation des labels\n",
    "with open(\"training_labels.json\", 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "for transcription_id in transcription_ids:\n",
    "    y.extend(labels[transcription_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séperation en Train et Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train, Z_valid, y_train, y_valid = train_test_split(Z, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train = Z_train[:,1]\n",
    "X_train = Z_train[:,0]\n",
    "\n",
    "A_valid = Z_valid[:,1]\n",
    "X_valid = Z_valid[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1589/1589 [00:06<00:00, 237.66it/s]\n",
      "Batches: 100%|██████████| 681/681 [00:02<00:00, 231.90it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = bert.encode(X_train, show_progress_bar=True)\n",
    "X_valid = bert.encode(X_valid, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train_df = pd.Series(A_train)\n",
    "A_valid_df = pd.Series(A_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = list(A_train_df.unique())\n",
    "dic = dict(zip(attributes, [str(i) for i in range(len(attributes))]))\n",
    "\n",
    "def b_dummies(x):\n",
    "    return dic.get(x, '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_col = [str(i) for i in range(-1,len(attributes))] # with -1\n",
    "\n",
    "A_train_df = A_train_df.apply(b_dummies)\n",
    "A_train_df = pd.get_dummies(A_train_df, dtype=float)\n",
    "A_train_df = A_train_df.reindex(columns=ordered_col, fill_value=0)\n",
    "A_train = A_train_df.values\n",
    "\n",
    "A_valid_df = A_valid_df.apply(b_dummies)\n",
    "A_valid_df = pd.get_dummies(A_valid_df, dtype=float)\n",
    "A_valid_df = A_valid_df.reindex(columns=ordered_col, fill_value=0)\n",
    "A_valid = A_valid_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train = np.concatenate([X_train, A_train], axis=1)\n",
    "Z_valid = np.concatenate([X_valid, A_valid], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele avec RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le f1_score est: 0.26717101333864224\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(Z_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(Z_valid)\n",
    "print(\"le f1_score est: \"+ str(f1_score(y_valid, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele avec DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21787, 402)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Dense(256, input_dim=402, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.3532 - accuracy: 0.8247 - val_loss: 0.3268 - val_accuracy: 0.8381\n",
      "Epoch 2/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.3378 - accuracy: 0.8327 - val_loss: 0.3248 - val_accuracy: 0.8399\n",
      "Epoch 3/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.3313 - accuracy: 0.8370 - val_loss: 0.3220 - val_accuracy: 0.8415\n",
      "Epoch 4/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.3264 - accuracy: 0.8390 - val_loss: 0.3217 - val_accuracy: 0.8422\n",
      "Epoch 5/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.3198 - accuracy: 0.8445 - val_loss: 0.3214 - val_accuracy: 0.8402\n",
      "Epoch 6/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.3144 - accuracy: 0.8473 - val_loss: 0.3221 - val_accuracy: 0.8396\n",
      "Epoch 7/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.3070 - accuracy: 0.8517 - val_loss: 0.3225 - val_accuracy: 0.8401\n",
      "Epoch 8/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.3001 - accuracy: 0.8574 - val_loss: 0.3256 - val_accuracy: 0.8415\n",
      "Epoch 9/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2942 - accuracy: 0.8597 - val_loss: 0.3311 - val_accuracy: 0.8384\n",
      "Epoch 10/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2858 - accuracy: 0.8654 - val_loss: 0.3290 - val_accuracy: 0.8373\n",
      "Epoch 11/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2791 - accuracy: 0.8687 - val_loss: 0.3316 - val_accuracy: 0.8339\n",
      "Epoch 12/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2709 - accuracy: 0.8752 - val_loss: 0.3401 - val_accuracy: 0.8347\n",
      "Epoch 13/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2638 - accuracy: 0.8768 - val_loss: 0.3413 - val_accuracy: 0.8330\n",
      "Epoch 14/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2555 - accuracy: 0.8862 - val_loss: 0.3466 - val_accuracy: 0.8320\n",
      "Epoch 15/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2508 - accuracy: 0.8854 - val_loss: 0.3625 - val_accuracy: 0.8292\n",
      "Epoch 16/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2448 - accuracy: 0.8915 - val_loss: 0.3663 - val_accuracy: 0.8337\n",
      "Epoch 17/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2380 - accuracy: 0.8961 - val_loss: 0.3778 - val_accuracy: 0.8290\n",
      "Epoch 18/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2332 - accuracy: 0.8966 - val_loss: 0.3773 - val_accuracy: 0.8291\n",
      "Epoch 19/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2286 - accuracy: 0.8998 - val_loss: 0.3793 - val_accuracy: 0.8300\n",
      "Epoch 20/20\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.2239 - accuracy: 0.9033 - val_loss: 0.3843 - val_accuracy: 0.8272\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_history = model.fit(Z_train, y_train, epochs=20, batch_size=32, validation_data=(Z_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681/681 [==============================] - 0s 629us/step\n",
      "0.5055176037834997\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(Z_valid)\n",
    "y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "\n",
    "print(f1_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, input_dim=402, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_0 = len(y_train) - np.sum(y_train)\n",
    "count_class_1 = np.sum(y_train)\n",
    "\n",
    "total = len(y_train)\n",
    "\n",
    "frequency_class_0 = count_class_0 / total\n",
    "frequency_class_1 = count_class_1 / total\n",
    "\n",
    "inverse_weight_class_0 = 1 / frequency_class_0\n",
    "inverse_weight_class_1 = 1 / frequency_class_1\n",
    "\n",
    "class_weights = {0: inverse_weight_class_0, 1: inverse_weight_class_1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589/1589 [==============================] - 4s 2ms/step - loss: 0.9501 - accuracy: 0.7161 - val_loss: 0.4413 - val_accuracy: 0.7395\n",
      "Epoch 2/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.9009 - accuracy: 0.7337 - val_loss: 0.4498 - val_accuracy: 0.7438\n",
      "Epoch 3/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8759 - accuracy: 0.7467 - val_loss: 0.4464 - val_accuracy: 0.7272\n",
      "Epoch 4/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8689 - accuracy: 0.7541 - val_loss: 0.4510 - val_accuracy: 0.7384\n",
      "Epoch 5/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8507 - accuracy: 0.7623 - val_loss: 0.4274 - val_accuracy: 0.7697\n",
      "Epoch 6/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8306 - accuracy: 0.7720 - val_loss: 0.4578 - val_accuracy: 0.7440\n",
      "Epoch 7/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8090 - accuracy: 0.7770 - val_loss: 0.4876 - val_accuracy: 0.7320\n",
      "Epoch 8/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.7931 - accuracy: 0.7885 - val_loss: 0.4295 - val_accuracy: 0.7706\n",
      "Epoch 9/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.7753 - accuracy: 0.7944 - val_loss: 0.4134 - val_accuracy: 0.7839\n",
      "Epoch 10/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.7575 - accuracy: 0.7971 - val_loss: 0.4367 - val_accuracy: 0.7743\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_history = model.fit(Z_train, y_train, epochs=10, batch_size=32, validation_data=(Z_valid, y_valid), class_weight= class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681/681 [==============================] - 0s 650us/step\n",
      "0.5651635720601238\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(Z_valid)\n",
    "y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "\n",
    "print(f1_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Z_train_scaled = scaler.fit_transform(Z_train)\n",
    "Z_valid_scaled = scaler.transform(Z_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1589/1589 [==============================] - 4s 2ms/step - loss: 1.0254 - accuracy: 0.7022 - val_loss: 0.4373 - val_accuracy: 0.7159\n",
      "Epoch 2/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.9138 - accuracy: 0.7249 - val_loss: 0.4507 - val_accuracy: 0.7148\n",
      "Epoch 3/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.9001 - accuracy: 0.7312 - val_loss: 0.4469 - val_accuracy: 0.7152\n",
      "Epoch 4/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8836 - accuracy: 0.7373 - val_loss: 0.4539 - val_accuracy: 0.7203\n",
      "Epoch 5/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8730 - accuracy: 0.7411 - val_loss: 0.4418 - val_accuracy: 0.7236\n",
      "Epoch 6/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8644 - accuracy: 0.7447 - val_loss: 0.4321 - val_accuracy: 0.7208\n",
      "Epoch 7/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8490 - accuracy: 0.7532 - val_loss: 0.4641 - val_accuracy: 0.7079\n",
      "Epoch 8/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8464 - accuracy: 0.7539 - val_loss: 0.4397 - val_accuracy: 0.7250\n",
      "Epoch 9/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8312 - accuracy: 0.7623 - val_loss: 0.4205 - val_accuracy: 0.7549\n",
      "Epoch 10/10\n",
      "1589/1589 [==============================] - 3s 2ms/step - loss: 0.8209 - accuracy: 0.7640 - val_loss: 0.4206 - val_accuracy: 0.7518\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, input_dim=402, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_history = model.fit(Z_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(Z_valid_scaled, y_valid), class_weight= class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681/681 [==============================] - 0s 626us/step\n",
      "0.5553819587205\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(Z_valid_scaled)\n",
    "y_pred = np.where(y_pred>=0.5, 1, 0)\n",
    "\n",
    "print(f1_score(y_valid, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
