{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training = Path(\"training\")\n",
    "path_to_test = Path(\"test\")\n",
    "\n",
    "# Recup des ids des diaoganls\n",
    "transcription_ids = []\n",
    "transcripts = path_to_training.glob('*.json')\n",
    "for transcript in transcripts:\n",
    "    transcription_ids.append(transcript.name[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les labels du training\n",
    "with open(\"training_labels.json\", 'r') as f:\n",
    "    transcription_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/dabereabasse/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "bert = SentenceTransformer('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définir les arêtes et les types d'arêtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_list(transcription_id, path_to_data):\n",
    "    discourse_graph = [] # list, i attribute j\n",
    "    with open(path_to_data / f\"{transcription_id}.txt\", 'r') as f:\n",
    "        for line in f:\n",
    "            tmp = line.split()\n",
    "            discourse_graph.append((int(tmp[0]), int(tmp[2]), tmp[1]))\n",
    "    return discourse_graph\n",
    "\n",
    "def get_replique(transcription_id, path_to_data):\n",
    "    with open(path_to_data / f\"{transcription_id}.json\", 'r') as f:\n",
    "        transcription = json.load(f)\n",
    "    ret = []\n",
    "    for i in range(len(transcription)):\n",
    "        text = transcription[i]['text']\n",
    "        ret.append(text)\n",
    "    return ret\n",
    "\n",
    "def create_data_object(transcription_id, labels=None, path_to_data = path_to_training):\n",
    "    edge_list = get_edge_list(transcription_id, path_to_data)\n",
    "    # Créer un graphe NetworkX\n",
    "    G = nx.DiGraph()\n",
    "    for src, dest, edge_type in edge_list:\n",
    "        G.add_edge(src, dest, relation=edge_type)\n",
    "\n",
    "    # Encoder les types d'arêtes\n",
    "    edge_types = list(set([edge[2] for edge in edge_list]))\n",
    "    edge_type_index = {edge_type: i for i, edge_type in enumerate(edge_types)}\n",
    "\n",
    "    # Préparer les données pour PyTorch Geometric\n",
    "    edges = []\n",
    "    edge_attrs = []\n",
    "    for src, dest, edge_type in G.edges(data='relation'):\n",
    "        edges.append((src, dest))\n",
    "        edge_attrs.append(edge_type_index[edge_type])\n",
    "\n",
    "    # Conversion en Tenseurs PyTorch\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "    x = torch.tensor(bert.encode(get_replique(transcription_id, path_to_data), show_progress_bar=True))\n",
    "    y = torch.tensor(labels) if labels else None\n",
    "    data = Data(edge_index=edge_index, edge_attr=edge_attr, x = x, y=y)\n",
    "\n",
    "    data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.train_mask[:int(0.8*data.num_nodes)] = True\n",
    "\n",
    "    data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.test_mask[int(0.8*data.num_nodes):] = True\n",
    "\n",
    "    data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    # Créer un objet Data pour PyTorch Geometric\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 24/24 [00:02<00:00,  9.33it/s]\n",
      "Batches: 100%|██████████| 41/41 [00:03<00:00, 11.22it/s]\n",
      "Batches: 100%|██████████| 11/11 [00:01<00:00,  9.81it/s]\n",
      "Batches: 100%|██████████| 18/18 [00:01<00:00,  9.69it/s]\n",
      "Batches: 100%|██████████| 13/13 [00:01<00:00,  9.97it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:02<00:00, 11.24it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:03<00:00, 11.66it/s]\n",
      "Batches: 100%|██████████| 29/29 [00:03<00:00,  8.82it/s]\n",
      "Batches: 100%|██████████| 38/38 [00:03<00:00, 11.28it/s]\n",
      "Batches: 100%|██████████| 19/19 [00:01<00:00, 10.68it/s]\n",
      "Batches: 100%|██████████| 31/31 [00:02<00:00, 10.73it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00,  9.58it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  8.25it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:02<00:00,  9.85it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:01<00:00, 11.94it/s]\n",
      "Batches: 100%|██████████| 36/36 [00:03<00:00, 11.39it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00,  8.93it/s]\n",
      "Batches: 100%|██████████| 23/23 [00:02<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 32/32 [00:02<00:00, 11.30it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:02<00:00,  9.79it/s]\n",
      "Batches: 100%|██████████| 30/30 [00:03<00:00,  9.33it/s]\n",
      "Batches: 100%|██████████| 23/23 [00:02<00:00, 10.36it/s]\n",
      "Batches: 100%|██████████| 29/29 [00:02<00:00,  9.94it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:01<00:00, 11.00it/s]\n",
      "Batches: 100%|██████████| 27/27 [00:02<00:00, 10.19it/s]\n",
      "Batches: 100%|██████████| 42/42 [00:04<00:00,  9.97it/s]\n",
      "Batches: 100%|██████████| 38/38 [00:03<00:00, 10.28it/s]\n",
      "Batches: 100%|██████████| 34/34 [00:03<00:00, 11.30it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:01<00:00, 11.26it/s]\n",
      "Batches: 100%|██████████| 29/29 [00:02<00:00, 12.11it/s]\n",
      "Batches: 100%|██████████| 18/18 [00:01<00:00, 12.24it/s]\n",
      "Batches: 100%|██████████| 30/30 [00:02<00:00, 11.69it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:01<00:00, 10.72it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 10.43it/s]\n",
      "Batches: 100%|██████████| 13/13 [00:01<00:00, 10.07it/s]\n",
      "Batches: 100%|██████████| 15/15 [00:01<00:00, 11.50it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:01<00:00, 10.79it/s]\n",
      "Batches: 100%|██████████| 47/47 [00:04<00:00, 11.62it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:02<00:00,  9.73it/s]\n",
      "Batches: 100%|██████████| 23/23 [00:02<00:00,  9.78it/s]\n",
      "Batches: 100%|██████████| 13/13 [00:01<00:00, 11.60it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:01<00:00, 12.12it/s]\n",
      "Batches: 100%|██████████| 31/31 [00:02<00:00, 10.62it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:01<00:00, 11.16it/s]\n",
      "Batches: 100%|██████████| 19/19 [00:01<00:00, 12.54it/s]\n",
      "Batches: 100%|██████████| 40/40 [00:03<00:00, 11.55it/s]\n",
      "Batches: 100%|██████████| 27/27 [00:02<00:00, 10.74it/s]\n",
      "Batches: 100%|██████████| 27/27 [00:02<00:00,  9.92it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:01<00:00, 11.43it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 13.58it/s]\n",
      "Batches: 100%|██████████| 27/27 [00:02<00:00, 11.49it/s]\n",
      "Batches: 100%|██████████| 37/37 [00:02<00:00, 12.59it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:02<00:00,  8.54it/s]\n",
      "Batches: 100%|██████████| 29/29 [00:02<00:00, 10.29it/s]\n",
      "Batches: 100%|██████████| 28/28 [00:02<00:00, 11.85it/s]\n",
      "Batches: 100%|██████████| 28/28 [00:02<00:00, 10.24it/s]\n",
      "Batches: 100%|██████████| 35/35 [00:03<00:00, 11.46it/s]\n",
      "Batches: 100%|██████████| 26/26 [00:02<00:00, 10.16it/s]\n",
      "Batches: 100%|██████████| 27/27 [00:02<00:00, 11.47it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:01<00:00, 11.44it/s]\n",
      "Batches: 100%|██████████| 23/23 [00:02<00:00,  9.94it/s]\n",
      "Batches: 100%|██████████| 29/29 [00:02<00:00, 10.19it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:02<00:00,  9.49it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00,  9.74it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:01<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 11/11 [00:01<00:00,  9.15it/s]\n",
      "Batches: 100%|██████████| 23/23 [00:02<00:00, 11.29it/s]\n",
      "Batches: 100%|██████████| 42/42 [00:03<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:01<00:00, 11.42it/s]\n",
      "Batches: 100%|██████████| 23/23 [00:02<00:00, 10.41it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 12.36it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:02<00:00,  8.86it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:02<00:00,  9.31it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:01<00:00, 10.36it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:01<00:00, 12.08it/s]\n",
      "Batches: 100%|██████████| 38/38 [00:03<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:02<00:00,  9.41it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:01<00:00, 12.33it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:02<00:00, 11.13it/s]\n",
      "Batches: 100%|██████████| 44/44 [00:03<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 34/34 [00:02<00:00, 11.62it/s]\n",
      "Batches: 100%|██████████| 18/18 [00:01<00:00,  9.92it/s]\n",
      "Batches: 100%|██████████| 15/15 [00:01<00:00, 10.33it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:02<00:00, 12.39it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:01<00:00, 11.30it/s]\n",
      "Batches: 100%|██████████| 68/68 [00:04<00:00, 14.06it/s]\n",
      "Batches: 100%|██████████| 20/20 [00:01<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 26/26 [00:02<00:00, 10.52it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 10.58it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:01<00:00, 12.62it/s]\n",
      "Batches: 100%|██████████| 28/28 [00:02<00:00, 11.83it/s]\n",
      "Batches: 100%|██████████| 28/28 [00:02<00:00, 11.57it/s]\n",
      "Batches: 100%|██████████| 11/11 [00:01<00:00,  8.74it/s]\n",
      "Batches: 100%|██████████| 33/33 [00:02<00:00, 11.75it/s]\n",
      "Batches: 100%|██████████| 36/36 [00:03<00:00, 11.75it/s]\n",
      "Batches: 100%|██████████| 14/14 [00:01<00:00,  9.83it/s]\n",
      "Batches: 100%|██████████| 18/18 [00:01<00:00,  9.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Créer un Data object pour chaque dialogue\n",
    "graphs_data = [create_data_object(dialogue, transcription_labels[dialogue]) for dialogue in transcription_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=[0,1], y= list(transcription_labels.values())[0])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1, 1], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.nn import GCNConv, global_add_pool, global_mean_pool, SAGPooling\n",
    "from torch.nn import Linear, BatchNorm1d\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = [400, 200, 100]\n",
    "p = [0.16496388, 0.49430627, 0.37350436]\n",
    "trashold = 0.6696969696969697\n",
    "\n",
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 512)\n",
    "\n",
    "        self.fc1 = Linear(512, 200)\n",
    "        self.fc2 = Linear(200, 100)\n",
    "\n",
    "        self.fc3 = Linear(100, 32)\n",
    "\n",
    "        self.classifier = Linear(32, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        ans = F.log_softmax(x, dim=1)\n",
    "        print(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "weight tensor should be defined either for all 1 classes or no classes but got weight tensor of shape: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/dabereabasse/Desktop/GitHub/inf554-extractive-summarization-2023/model5.ipynb Cellule 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dabereabasse/Desktop/GitHub/inf554-extractive-summarization-2023/model5.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dabereabasse/Desktop/GitHub/inf554-extractive-summarization-2023/model5.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m out \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dabereabasse/Desktop/GitHub/inf554-extractive-summarization-2023/model5.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[data\u001b[39m.\u001b[39;49mtrain_mask], data\u001b[39m.\u001b[39;49my[data\u001b[39m.\u001b[39;49mtrain_mask])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dabereabasse/Desktop/GitHub/inf554-extractive-summarization-2023/model5.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dabereabasse/Desktop/GitHub/inf554-extractive-summarization-2023/model5.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: weight tensor should be defined either for all 1 classes or no classes but got weight tensor of shape: [2]"
     ]
    }
   ],
   "source": [
    "NUM_FEATURES = graphs_data[0].x.shape[1]\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCNModel(num_features=NUM_FEATURES, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for data in graphs_data:  # Remplacez par vos données d'entraînement\n",
    "        data = data.to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch}, Loss: {total_loss / len(graphs_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluer le Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for data in graphs_data:\n",
    "    data = data.to(device)\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    pred = pred[data.test_mask].cpu().numpy()\n",
    "    true_labels = data.y[data.test_mask].cpu().numpy()\n",
    "    all_preds.extend(pred)\n",
    "    all_labels.extend(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.3625980060013552\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(all_labels, all_preds)\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGwCAYAAAA0bWYRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCHElEQVR4nO3deVhU5f8//uewDPsMogISixipkLhh4rxLkyLRrDS13hYVmctHA0vM9VfuKX01cynTyhItLW3Rt2JqpLmjJka5IKVioDBgIYygMDBzfn8QJyeZnHFmWOY8H9d1rss55z73eR0y58Xrvs99ZIIgCCAiIiLJcmjsAIiIiKhxMRkgIiKSOCYDREREEsdkgIiISOKYDBAREUkckwEiIiKJYzJAREQkcU6NHYAl9Ho9CgoK4OXlBZlM1tjhEBGRmQRBwLVr1xAQEAAHB9v9flpZWQmtVmtxP3K5HK6urlaIqGlp1slAQUEBgoKCGjsMIiKyUH5+PgIDA23Sd2VlJUJDPKEu1lncl7+/P3Jzc+0uIWjWyYCXlxcA4MSPreHpyREPsk+tHT0aOwQim9GU6xHS/aL477ktaLVaqIt1+D2zLRRed/5dobmmR0jURWi1WiYDTUnd0ICnpwO8LPgPTNSUKRz5d5vsX0MM9Xp6yeDpdefX0cN+h6ObdTJARERkKp2gh86Ct/HoBL31gmlimAwQEZEk6CFAjzvPBiw5t6lj/ZGIiEjiWBkgIiJJ0EMPSwr9lp3dtDEZICIiSdAJAnTCnZf6LTm3qeMwARERkcSxMkBERJLACYTGMRkgIiJJ0EOAjslAvThMQEREJHGsDBARkSRwmMA4JgNERCQJfJrAOA4TEBERSRwrA0REJAn6vzZLzrdXrAwQEZEk6P56msCSzVyXL1/Gc889h5YtW8LNzQ2RkZE4fvy4eFwQBMycORNt2rSBm5sbYmNj8dtvvxn0UVJSgvj4eCgUCnh7e2PkyJEoLy83aPPLL7+gd+/ecHV1RVBQEBYuXGhWnEwGiIhIEnSC5Zs5rl69ivvvvx/Ozs7YsWMHzpw5g8WLF6NFixZim4ULF2L58uVYtWoVjh49Cg8PD8TFxaGyslJsEx8fj9OnTyM9PR1paWnYv38/xowZIx7XaDTo168fQkJCkJmZiUWLFmH27Nn48MMPTY5VJgjNd0aERqOBUqnEr9l+8PJiXkP2ydfRo7FDILIZzTU9WrS/gLKyMigUCttc46/vil/O+Fr0XXHtmh6dI4pNjnXatGk4dOgQDhw4UO9xQRAQEBCA1157DZMmTQIAlJWVwc/PD6mpqRg+fDiys7MRERGBH3/8ET169AAA7Ny5E48++iguXbqEgIAArFy5Eq+//jrUajXkcrl47S1btuDs2bMm3Ru/QYmISBL0VtiA2uTi5q2qqqre623duhU9evTAU089BV9fX3Tr1g0fffSReDw3NxdqtRqxsbHiPqVSiejoaGRkZAAAMjIy4O3tLSYCABAbGwsHBwccPXpUbNOnTx8xEQCAuLg45OTk4OrVqyb9bJgMEBGRJOghg86CTQ8ZACAoKAhKpVLcUlJS6r3ehQsXsHLlStxzzz3YtWsXxo0bh1deeQVr164FAKjVagCAn5+fwXl+fn7iMbVaDV9fX4PjTk5O8PHxMWhTXx83X+N2+DQBERGRGfLz8w2GCVxcXOptp9fr0aNHDyxYsAAA0K1bN5w6dQqrVq1CQkJCg8RqKlYGiIhIEvSC5RsAKBQKg81YMtCmTRtEREQY7AsPD0deXh4AwN/fHwBQVFRk0KaoqEg85u/vj+LiYoPjNTU1KCkpMWhTXx83X+N2mAwQEZEkWDJEULeZ4/7770dOTo7Bvl9//RUhISEAgNDQUPj7+2P37t3icY1Gg6NHj0KlUgEAVCoVSktLkZmZKbbZs2cP9Ho9oqOjxTb79+9HdXW12CY9PR0dOnQweHLh3zAZICIisoHk5GQcOXIECxYswLlz57BhwwZ8+OGHSExMBADIZDJMmDABb775JrZu3YqTJ0/ihRdeQEBAAAYPHgygtpLQv39/jB49GseOHcOhQ4eQlJSE4cOHIyAgAADw7LPPQi6XY+TIkTh9+jQ2btyIZcuWYeLEiSbHyjkDREQkCXfy2/0/zzfHfffdh82bN2P69OmYO3cuQkNDsXTpUsTHx4ttpkyZgoqKCowZMwalpaV44IEHsHPnTri6uopt1q9fj6SkJDz88MNwcHDA0KFDsXz5cvG4UqnEd999h8TERERFRaFVq1aYOXOmwVoEt8N1BoiaOK4zQPasIdcZOHgqAJ4WfFeUX9PjgU4FNo21sfAblIiISOI4TEBERJLQ0MMEzQmTASIikgQdHKCzoCCus2IsTQ2TASIikgRBkEEv3Plv94IF5zZ1nDNAREQkcawMEBGRJHDOgHFMBoiISBJ0ggN0ggVzBprtg/i3x2ECIiIiiWNlgIiIJEEPGfQW/A6sh/2WBpgMEBGRJHDOgHEcJiAiIpI4VgaIiEgSLJ9AyGECIiKiZq12zsCdl/otObep4zABERGRxLEyQEREkqC38N0EfJqAiIiomeOcAeOYDBARkSTo4cB1BozgnAEiIiKJY2WAiIgkQSfIoLPgNcSWnNvUMRkgIiJJ0Fk4gVDHYQIiIiKyV6wMEBGRJOgFB+gteJpAz6cJiIiImjcOExjHYQIiIiKJY2WAiIgkQQ/LngjQWy+UJofJABERSYLliw7ZbzHdfu+MiIiITMLKABERSYLl7yaw39+fmQwQEZEk6CGDHpbMGeAKhERERM0aKwPG2e+dERERkUlYGSAiIkmwfNEh+/39mckAERFJgl6QQW/JOgN2/NZC+01ziIiIyCSsDBARkSToLRwmsOdFh5gMEBGRJFj+1kL7TQbs986IiIjIJKwMEBGRJOggg86ChYMsObepYzJARESSwGEC4+z3zoiIiMgkrAwQEZEk6GBZqV9nvVCaHCYDREQkCRwmMI7JABERSQJfVGSc/d4ZERERmYSVASIikgQBMugtmDMg8NFCIiKi5o3DBMbZ750RERGRSVgZICIiSeArjI1jMkBERJKgs/CthZac29TZ750RERGRSZgMEBGRJNQNE1iymWP27NmQyWQGW8eOHcXjlZWVSExMRMuWLeHp6YmhQ4eiqKjIoI+8vDwMHDgQ7u7u8PX1xeTJk1FTU2PQZu/evejevTtcXFwQFhaG1NRUs382TAaIiEgS9HCweDPXvffei8LCQnE7ePCgeCw5ORnbtm3Dl19+iX379qGgoABDhgwRj+t0OgwcOBBarRaHDx/G2rVrkZqaipkzZ4ptcnNzMXDgQMTExCArKwsTJkzAqFGjsGvXLrPi5JwBIiIiG3FycoK/v/8t+8vKyvDxxx9jw4YNeOihhwAAa9asQXh4OI4cOYJevXrhu+++w5kzZ/D999/Dz88PXbt2xbx58zB16lTMnj0bcrkcq1atQmhoKBYvXgwACA8Px8GDB7FkyRLExcWZHCcrA0REJAk6QWbxBgAajcZgq6qqMnrN3377DQEBAWjXrh3i4+ORl5cHAMjMzER1dTViY2PFth07dkRwcDAyMjIAABkZGYiMjISfn5/YJi4uDhqNBqdPnxbb3NxHXZu6PkzFZICIiCTBWnMGgoKCoFQqxS0lJaXe60VHRyM1NRU7d+7EypUrkZubi969e+PatWtQq9WQy+Xw9vY2OMfPzw9qtRoAoFarDRKBuuN1x/6tjUajwY0bN0z+2XCYgIiIJEGw8K2Fwl/n5ufnQ6FQiPtdXFzqbT9gwADxz507d0Z0dDRCQkKwadMmuLm53XEctsDKABERkRkUCoXBZiwZ+Cdvb2+0b98e586dg7+/P7RaLUpLSw3aFBUViXMM/P39b3m6oO7z7dooFAqzEg4mA0REJAk6yCzeLFFeXo7z58+jTZs2iIqKgrOzM3bv3i0ez8nJQV5eHlQqFQBApVLh5MmTKC4uFtukp6dDoVAgIiJCbHNzH3Vt6vowFZMBIiKSBL1g6bwB8643adIk7Nu3DxcvXsThw4fx5JNPwtHREc888wyUSiVGjhyJiRMn4ocffkBmZiZGjBgBlUqFXr16AQD69euHiIgIPP/88/j555+xa9cuvPHGG0hMTBSrEWPHjsWFCxcwZcoUnD17Fu+//z42bdqE5ORks2LlnAEiIiIbuHTpEp555hn8+eefaN26NR544AEcOXIErVu3BgAsWbIEDg4OGDp0KKqqqhAXF4f3339fPN/R0RFpaWkYN24cVCoVPDw8kJCQgLlz54ptQkNDsX37diQnJ2PZsmUIDAzE6tWrzXqsEABkgiCYmes0HRqNBkqlEr9m+8HLi0UOU5UUyrExJQS//NACVTcc4Ne2EqMXn0O7LuUAgB93+GDPp/64eNIT5aXOeHNnFkLurTDoQ1spw4Z5oTi6tRWqtQ6IfPAqXpx/AcrW1Qbt9m/yxc6PAqDOdYOrZw16DvwTL86/0GD3ag98HT0aO4Rm549CZ3w8vw1+/EGBqhsOCGhbhdeW5KF9l9rZ1Z++7Y+9//PGlQJnOMsFhEXewIhphejY/ToAQJ0vx4Ylfsg65ImrV5zR0q8aDw25imdeLYKz/O9/Mo/v9cKnb/vj9xxXyF0EdOpVjjGzCuAfpG2U+26ONNf0aNH+AsrKygwm5Vn1Gn99VyT8MBxyT/kd96Mt12JtzBc2jbWxsDIgMRWljpg3JBLhqjJMWncGXi2rUZTrBg/l38tbVl13RPue1xD9+J/4eEpYvf2snxOKn/f4IGlVDty9arBuRjssG9MRMzefFNvs+DAAOz4MwPDXL+LubuWouuGAP/JdbX6PJG3XSh0xcdA96Pyfa3jzswvwblmDyxdc4KnUiW3ualeJxPmX0CZEi6pKB2z+sDWmP3M31hw+A++WOuSfc4FeD7z6/y4hILQKF8+6YunkIFRed8CYWQUAAHWeHLNHhGLImCuY+t7vqNA44oPZd2HeyLZY8d2vjXX79C/0kEFvwbi/Jec2dU0iGVixYgUWLVoEtVqNLl264N1330XPnj0bOyy7lLYyED5tqjDmnXPiPt9gwwUzHhh6BQBwJb/+GbLXNY7Yt9EPL7/7K+69vwwAMHrxOUyN6Y5zJzwR1r0cFaWO+GpRMCauyca9D5SJ5waHX7f2LREZ2LTCF60CtJi0NF/c5x9s+Jv6Q0NKDT6PmX0ZOz9vidwzbujWuxz3xVzDfTHXxONtQrS4dL4YaetaicnAb7+4Qa+T4cWphXD4qzA5bGwxZo8IRU014ORsm/sjsoVGr61v3LgREydOxKxZs3DixAl06dIFcXFxBrMnyXpOpPsgtHMFlo/tgJe73oc3+nfBDxv8bn/iTXJPekJX7YB7HygV9wWE3UDLuyrxW2Zt6ezUAW8IggxX1XJMjemGV+7rgXfHdcCfBXdeoiMyxZHvlGjf5TreHNMWT0fei5cfaY9v1/sYbV+tleHbz1rCQ6FDuwjji7RUXHOEl/ff1YV7Ot+Ag4OA777wgU4HVGgc8P3XLdCt9zUmAk2UtVYgtEeNngy88847GD16NEaMGIGIiAisWrUK7u7u+OSTTxo7NLt0Jc8Vez7zh3/bG5jy2Rk89Lwan84MxYEvW5vcR1mxM5zkenjcVHYFAGWrapRdqf1XsDjPFXo9sPW9QMTPysUrH5xFRakT/t+z96JGa7//Q1HjK8yTI21dKwSEVmHBhgt4LOFPrJwRiPRNLQzaHUlXYFBYJB4P7YzNH7VGyhfnoGypq7fPy7ly/O+T1nj0+T/Eff7BWiz4/DzWvNUGj7XtgiEdO+OPAjle/+B3m94f3Tn9X4sOWbLZq0a9M61Wi8zMTIN1lR0cHBAbG1vvuspVVVW3rAlN5tHrgZBO5Xh6Wh7adqrAQ/FF6PtsEfZ8duuLNCwh6GXQVTvg+Tm56Ny3FGHdy/HyezlQ57rhzGGlVa9FdDNBD4R1uoGXphciLPIGHn3uTwx49k9s/7SVQbuu95fj/fQcLNn6G3r0vYb5/9cWpX/cOnL6R6EzXo+/G30eK8Wj8SXi/pJiJyydHIRHnirBu9/+ire/+Q3OcgHzRrdF852WTVLVqMnAH3/8AZ1OV++6ynXrLt8sJSXFYD3ooKCghgrVbnj7anHXPYal0ICwG/jzsmkraAGA0rcaNVoHVJQ5Guwv+8NZfJrA27d2jPau9n/PEVC0rIGXTzX+LDD9WkTm8vGtQUj7SoN9QfdUoviyYe3e1V2Pu0K1CI+6jonv5MPRCdj5ueFwwp9qJ0x56m5E9KjAq4vyDY5tS20FDy89Rs2oTToie1Vgyru/I+ugF86ecLfNzZFF9LDw3QR2PIGwWdU8pk+fjrKyMnHLz8+//UlkoH2Payg8bzijX33BDS0Djb91659CI8vh6KzHmUPe4r7C827487Ir7omqrdbcc59G3F+n/KoTrpU4o9Vdpl+LyFwR91Ug/7xhwnn5ggt876o2ckYtQQ9UV/39T+Ifhc6YPCwM90TewGtL8sRJgnUqbzhA5mBYAnBwrP2s11twA2Qzwl9PE9zpJjAZsI1WrVrB0dGx3nWV63v/s4uLyy1rQpN5+o8qwPmfvLD13UAU5bri8OZW+GGDH2IT/q7ElF91wu+nPXD5t9rfbgrPu+H30x4oLa79zcpdocOD/y3C+rltceawErm/eODD18IQFqVBWPfatQratKtE935/4tNZofj1uBfyz7rjg4n3ICDsBsL/U3ZrYERWMmRMMc6e8MDny31xOVeOPd9449vPWuKJEbXj/ZXXHfBJShtkZ7qj6JIzfvvFDYuTg/CH2hm9Hy8F8Hci0DqgGqNnFqDsTyeUFNdudaIf1uDXLHd89o4fLl+Q/9VPMPwCtQjrZPrb4qjhWOuthfao0Rcdio6ORs+ePfHuu+8CAPR6PYKDg5GUlIRp06b967lcdOjO/PR9C2x6KwRFF93QOqgS/UcXIObZvxOy/Zt88dFr99xy3pPJeRgysbYaU7fo0JH/1S461PnBUiTMPw9v379/+7pxzRGfzQnF8Z0t4SAT0LGXBs/NuYCWAVyQxRxcdMh8R9IVWJPSBpdzXeAfpMWQ/ysWx/u1lTK8lRiCsz+5Q1PiBK8WOrTvch3PTlCjQ9faL/HvNvpgcXJwvX3vKsgS/7x3ize+fN8Xly64wMVNj/Co6xj5egGC72H1y1QNuejQ0O8T4Oxx5080VVdo8XXsWrtcdKjRk4GNGzciISEBH3zwAXr27ImlS5di06ZNOHv27C1zCf6JyQBJAZMBsmcNmQw8mT7C4mRg8yNr7DIZaPRFh/773//iypUrmDlzJtRqNbp27YqdO3feNhEgIiIyh6WlfnseJmj0ZAAAkpKSkJSU1NhhEBERSVKTSAaIiIhsje8mMI7JABERSQKHCYzjrDsiIiKJY2WAiIgkgZUB45gMEBGRJDAZMI7DBERERBLHygAREUkCKwPGMRkgIiJJEGDZ44H2/GZqJgNERCQJrAwYxzkDREREEsfKABERSQIrA8YxGSAiIklgMmAchwmIiIgkjpUBIiKSBFYGjGMyQEREkiAIMggWfKFbcm5Tx2ECIiIiiWNlgIiIJEEPmUWLDllyblPHZICIiCSBcwaM4zABERGRxLEyQEREksAJhMYxGSAiIkngMIFxTAaIiEgSWBkwjnMGiIiIJI6VASIikgTBwmECe64MMBkgIiJJEAAIgmXn2ysOExAREUkcKwNERCQJesgg4wqE9WIyQEREksCnCYzjMAEREZHEsTJARESSoBdkkHHRoXoxGSAiIkkQBAufJrDjxwk4TEBERCRxrAwQEZEkcAKhcUwGiIhIEpgMGMdkgIiIJIETCI3jnAEiIiKJY2WAiIgkgU8TGMdkgIiIJKE2GbBkzoAVg2liOExARERkY2+99RZkMhkmTJgg7qusrERiYiJatmwJT09PDB06FEVFRQbn5eXlYeDAgXB3d4evry8mT56MmpoagzZ79+5F9+7d4eLigrCwMKSmppodH5MBIiKShLqnCSzZ7sSPP/6IDz74AJ07dzbYn5ycjG3btuHLL7/Evn37UFBQgCFDhojHdTodBg4cCK1Wi8OHD2Pt2rVITU3FzJkzxTa5ubkYOHAgYmJikJWVhQkTJmDUqFHYtWuXWTEyGSAiIkkQrLCZq7y8HPHx8fjoo4/QokULcX9ZWRk+/vhjvPPOO3jooYcQFRWFNWvW4PDhwzhy5AgA4LvvvsOZM2fw2WefoWvXrhgwYADmzZuHFStWQKvVAgBWrVqF0NBQLF68GOHh4UhKSsKwYcOwZMkSs+JkMkBERGQGjUZjsFVVVRltm5iYiIEDByI2NtZgf2ZmJqqrqw32d+zYEcHBwcjIyAAAZGRkIDIyEn5+fmKbuLg4aDQanD59Wmzzz77j4uLEPkzFZICIiCTBWsMEQUFBUCqV4paSklLv9b744gucOHGi3uNqtRpyuRze3t4G+/38/KBWq8U2NycCdcfrjv1bG41Ggxs3bpj8s+HTBEREJA13Wuu/+XwA+fn5UCgU4m4XF5dbmubn5+PVV19Feno6XF1dLbhow2BlgIiIpMHSqsBflQGFQmGw1ZcMZGZmori4GN27d4eTkxOcnJywb98+LF++HE5OTvDz84NWq0VpaanBeUVFRfD39wcA+Pv73/J0Qd3n27VRKBRwc3Mz+UfDZICIiMjKHn74YZw8eRJZWVni1qNHD8THx4t/dnZ2xu7du8VzcnJykJeXB5VKBQBQqVQ4efIkiouLxTbp6elQKBSIiIgQ29zcR12buj5MxWECIiKShIZcgdDLywudOnUy2Ofh4YGWLVuK+0eOHImJEyfCx8cHCoUC48ePh0qlQq9evQAA/fr1Q0REBJ5//nksXLgQarUab7zxBhITE8VqxNixY/Hee+9hypQpeOmll7Bnzx5s2rQJ27dvN+vemAwQEZEkNLW3Fi5ZsgQODg4YOnQoqqqqEBcXh/fff1887ujoiLS0NIwbNw4qlQoeHh5ISEjA3LlzxTahoaHYvn07kpOTsWzZMgQGBmL16tWIi4szKxaZIDTfBRY1Gg2USiV+zfaDlxdHPMg++Tp6NHYIRDajuaZHi/YXUFZWZjApz6rX+Ou7ou0nb8DB/c4n8+mvV+LiS2/aNNbGwsoAERFJw02TAO/4fDvFZICIiCSBby00jrV1IiIiiWNlgIiIpMFKiw7ZIyYDREQkCU3taYKmxKRkYOvWrSZ3+MQTT9xxMERERNTwTEoGBg8ebFJnMpkMOp3OkniIiIhsx45L/ZYwKRnQ6/W2joOIiMimOExgnEVPE1RWVlorDiIiItsSrLDZKbOTAZ1Oh3nz5uGuu+6Cp6cnLly4AACYMWMGPv74Y6sHSERERLZldjIwf/58pKamYuHChZDL5eL+Tp06YfXq1VYNjoiIyHpkVtjsk9nJwLp16/Dhhx8iPj4ejo6O4v4uXbrg7NmzVg2OiIjIajhMYJTZycDly5cRFhZ2y369Xo/q6mqrBEVEREQNx+xkICIiAgcOHLhl/1dffYVu3bpZJSgiIiKrY2XAKLNXIJw5cyYSEhJw+fJl6PV6fPPNN8jJycG6deuQlpZmixiJiIgsx7cWGmV2ZWDQoEHYtm0bvv/+e3h4eGDmzJnIzs7Gtm3b8Mgjj9giRiIiIrKhO3o3Qe/evZGenm7tWIiIiGyGrzA27o5fVHT8+HFkZ2cDqJ1HEBUVZbWgiIiIrI5vLTTK7GTg0qVLeOaZZ3Do0CF4e3sDAEpLS/Gf//wHX3zxBQIDA60dIxEREdmQ2XMGRo0aherqamRnZ6OkpAQlJSXIzs6GXq/HqFGjbBEjERGR5eomEFqy2SmzKwP79u3D4cOH0aFDB3Ffhw4d8O6776J3795WDY6IiMhaZELtZsn59srsZCAoKKjexYV0Oh0CAgKsEhQREZHVcc6AUWYPEyxatAjjx4/H8ePHxX3Hjx/Hq6++irffftuqwREREZHtmVQZaNGiBWSyv8dKKioqEB0dDSen2tNramrg5OSEl156CYMHD7ZJoERERBbhokNGmZQMLF261MZhEBER2RiHCYwyKRlISEiwdRxERETUSO540SEAqKyshFarNdinUCgsCoiIiMgmWBkwyuwJhBUVFUhKSoKvry88PDzQokULg42IiKhJ4lsLjTI7GZgyZQr27NmDlStXwsXFBatXr8acOXMQEBCAdevW2SJGIiIisiGzhwm2bduGdevWoW/fvhgxYgR69+6NsLAwhISEYP369YiPj7dFnERERJbh0wRGmV0ZKCkpQbt27QDUzg8oKSkBADzwwAPYv3+/daMjIiKykroVCC3Z7JXZyUC7du2Qm5sLAOjYsSM2bdoEoLZiUPfiIiIiImo+zE4GRowYgZ9//hkAMG3aNKxYsQKurq5ITk7G5MmTrR4gERGRVXACoVFmzxlITk4W/xwbG4uzZ88iMzMTYWFh6Ny5s1WDIyIiItuzaJ0BAAgJCUFISIg1YiEiIrIZGSx8a6HVIml6TEoGli9fbnKHr7zyyh0HQ0RERA3PpGRgyZIlJnUmk8kaJRkYE94LTjLnBr8uUUMofyq6sUMgspma6koAMxrmYny00CiTkoG6pweIiIiaLS5HbJTZTxMQERGRfbF4AiEREVGzwMqAUUwGiIhIEixdRZArEBIREZHdYmWAiIikgcMERt1RZeDAgQN47rnnoFKpcPnyZQDAp59+ioMHD1o1OCIiIqvhcsRGmZ0MfP3114iLi4Obmxt++uknVFVVAQDKysqwYMECqwdIREREtmV2MvDmm29i1apV+Oijj+Ds/PdCP/fffz9OnDhh1eCIiIisha8wNs7sOQM5OTno06fPLfuVSiVKS0utERMREZH1cQVCo8yuDPj7++PcuXO37D948CDatWtnlaCIiIisjnMGjDI7GRg9ejReffVVHD16FDKZDAUFBVi/fj0mTZqEcePG2SJGIiIisiGzhwmmTZsGvV6Phx9+GNevX0efPn3g4uKCSZMmYfz48baIkYiIyGJcdMg4sysDMpkMr7/+OkpKSnDq1CkcOXIEV65cwbx582wRHxERkXU08DDBypUr0blzZygUCigUCqhUKuzYsUM8XllZicTERLRs2RKenp4YOnQoioqKDPrIy8vDwIED4e7uDl9fX0yePBk1NTUGbfbu3Yvu3bvDxcUFYWFhSE1NNS9QWLACoVwuR0REBHr27AlPT8877YaIiMguBQYG4q233kJmZiaOHz+Ohx56CIMGDcLp06cBAMnJydi2bRu+/PJL7Nu3DwUFBRgyZIh4vk6nw8CBA6HVanH48GGsXbsWqampmDlzptgmNzcXAwcORExMDLKysjBhwgSMGjUKu3btMitWmSAIZuU6MTExkMmMz6jcs2ePWQFYQqPRQKlUoi8GwUnmfPsTiJqh8qeiGzsEIpupqa7E8c0zUFZWBoVCYZNr1H1XtJuxAI6urnfcj66yEhfm/X8Wxerj44NFixZh2LBhaN26NTZs2IBhw4YBAM6ePYvw8HBkZGSgV69e2LFjBx577DEUFBTAz88PALBq1SpMnToVV65cgVwux9SpU7F9+3acOnVKvMbw4cNRWlqKnTt3mhyX2ZWBrl27okuXLuIWEREBrVaLEydOIDIy0tzuiIiIGoaVhgk0Go3BVrf43r/R6XT44osvUFFRAZVKhczMTFRXVyM2NlZs07FjRwQHByMjIwMAkJGRgcjISDERAIC4uDhoNBqxupCRkWHQR12buj5MZfYEwiVLltS7f/bs2SgvLze3OyIiomYlKCjI4POsWbMwe/bsetuePHkSKpUKlZWV8PT0xObNmxEREYGsrCzI5XJ4e3sbtPfz84NarQYAqNVqg0Sg7njdsX9ro9FocOPGDbi5uZl0T1Z7UdFzzz2Hnj174u2337ZWl0RERNZjpRcV5efnGwwTuLi4GD2lQ4cOyMrKQllZGb766iskJCRg3759FgRhG1ZLBjIyMuBqwVgMERGRLVnr0cK6pwNMIZfLERYWBgCIiorCjz/+iGXLluG///0vtFotSktLDaoDRUVF8Pf3B1C7yN+xY8cM+qt72uDmNv98AqGoqAgKhcLkqgBwB8nAzTMdAUAQBBQWFuL48eOYMWOGud0RERFJhl6vR1VVFaKiouDs7Izdu3dj6NChAGqX+8/Ly4NKpQIAqFQqzJ8/H8XFxfD19QUApKenQ6FQICIiQmzz7bffGlwjPT1d7MNUZicDSqXS4LODgwM6dOiAuXPnol+/fuZ2R0REZJemT5+OAQMGIDg4GNeuXcOGDRuwd+9e7Nq1C0qlEiNHjsTEiRPh4+MDhUKB8ePHQ6VSoVevXgCAfv36ISIiAs8//zwWLlwItVqNN954A4mJieLQxNixY/Hee+9hypQpeOmll7Bnzx5s2rQJ27dvNytWs5IBnU6HESNGIDIyEi1atDDrQkRERI3KSnMGTFVcXIwXXngBhYWFUCqV6Ny5M3bt2oVHHnkEQO2EfAcHBwwdOhRVVVWIi4vD+++/L57v6OiItLQ0jBs3DiqVCh4eHkhISMDcuXPFNqGhodi+fTuSk5OxbNkyBAYGYvXq1YiLizMrVrPXGXB1dUV2djZCQ0PNupAtcJ0BkgKuM0D2rCHXGQibZvk6A+fesmydgabK7HUGOnXqhAsXLtgiFiIiImoEZicDb775JiZNmoS0tDQUFhbesvgCERFRk8XXF9fL5DkDc+fOxWuvvYZHH30UAPDEE08YLEssCAJkMhl0Op31oyQiIrJUA88ZaE5MTgbmzJmDsWPH4ocffrBlPERERNTATE4G6uYZPvjggzYLhoiIyFasteiQPTLr0cJ/e1shERFRk8ZhAqPMSgbat29/24SgpKTEooCIiIioYZmVDMyZM+eWFQiJiIiaAw4TGGdWMjB8+HBxfWQiIqJmhcMERpm8zgDnCxAREdkns58mICIiapZYGTDK5GRAr9fbMg4iIiKb4pwB48x+hTEREVGzxMqAUWa/m4CIiIjsCysDREQkDawMGMVkgIiIJIFzBozjMAEREZHEsTJARETSwGECo5gMEBGRJHCYwDgOExAREUkcKwNERCQNHCYwiskAERFJA5MBozhMQEREJHGsDBARkSTI/tosOd9eMRkgIiJp4DCBUUwGiIhIEvhooXGcM0BERCRxrAwQEZE0cJjAKCYDREQkHXb8hW4JDhMQERFJHCsDREQkCZxAaByTASIikgbOGTCKwwREREQSx8oAERFJAocJjGMyQERE0sBhAqM4TEBERCRxrAwQEZEkcJjAOCYDREQkDRwmMIrJABERSQOTAaM4Z4CIiEjiWBkgIiJJ4JwB45gMEBGRNHCYwCgOExAREUkcKwNERCQJMkGATLjzX+8tObepYzJARETSwGECozhMQEREJHGsDBARkSTwaQLjmAwQEZE0cJjAKA4TEBERSRwrA0REJAkcJjCOlQEiIpIGwQqbGVJSUnDffffBy8sLvr6+GDx4MHJycgzaVFZWIjExES1btoSnpyeGDh2KoqIigzZ5eXkYOHAg3N3d4evri8mTJ6Ompsagzd69e9G9e3e4uLggLCwMqampZsXKZICIiCShrjJgyWaOffv2ITExEUeOHEF6ejqqq6vRr18/VFRUiG2Sk5Oxbds2fPnll9i3bx8KCgowZMgQ8bhOp8PAgQOh1Wpx+PBhrF27FqmpqZg5c6bYJjc3FwMHDkRMTAyysrIwYcIEjBo1Crt27TLjZyM031UUNBoNlEol+mIQnGTOjR0OkU2UPxXd2CEQ2UxNdSWOb56BsrIyKBQKm1yj7rsi6r/z4Sh3veN+dNpKZG58/Y5jvXLlCnx9fbFv3z706dMHZWVlaN26NTZs2IBhw4YBAM6ePYvw8HBkZGSgV69e2LFjBx577DEUFBTAz88PALBq1SpMnToVV65cgVwux9SpU7F9+3acOnVKvNbw4cNRWlqKnTt3mhQbKwNERCQNVhom0Gg0BltVVZVJly8rKwMA+Pj4AAAyMzNRXV2N2NhYsU3Hjh0RHByMjIwMAEBGRgYiIyPFRAAA4uLioNFocPr0abHNzX3UtanrwxRMBoiISDKsMUQQFBQEpVIpbikpKbe9rl6vx4QJE3D//fejU6dOAAC1Wg25XA5vb2+Dtn5+flCr1WKbmxOBuuN1x/6tjUajwY0bN0z6ufBpAiIiIjPk5+cbDBO4uLjc9pzExEScOnUKBw8etGVod4zJABERSYMg1G6WnA9AoVCYNWcgKSkJaWlp2L9/PwIDA8X9/v7+0Gq1KC0tNagOFBUVwd/fX2xz7Ngxg/7qnja4uc0/n0AoKiqCQqGAm5ubSTFymICIiCShoZ8mEAQBSUlJ2Lx5M/bs2YPQ0FCD41FRUXB2dsbu3bvFfTk5OcjLy4NKpQIAqFQqnDx5EsXFxWKb9PR0KBQKREREiG1u7qOuTV0fpmBlgIiIyAYSExOxYcMG/O9//4OXl5c4xq9UKuHm5galUomRI0di4sSJ8PHxgUKhwPjx46FSqdCrVy8AQL9+/RAREYHnn38eCxcuhFqtxhtvvIHExERxeGLs2LF47733MGXKFLz00kvYs2cPNm3ahO3bt5scK5MBIiKShgZ+N8HKlSsBAH379jXYv2bNGrz44osAgCVLlsDBwQFDhw5FVVUV4uLi8P7774ttHR0dkZaWhnHjxkGlUsHDwwMJCQmYO3eu2CY0NBTbt29HcnIyli1bhsDAQKxevRpxcXEmx8pkgIiIJEGmr90sOd8cpizj4+rqihUrVmDFihVG24SEhODbb7/913769u2Ln376ybwAb8I5A0RERBLHyoDE/TepCPc/WoagsCpoKx1w5rg7Pp7fBpfO/71K18KvzqHLfyoMztu+riWWT6udFfvI0yWYtDS/3v6fjoxA2Z9cHZIaTpe7C/HsQz+jY9AfaKW8jmmr++HAybbicTd5NcY9fhS9O/8OpXslCkq88NX+TthyqHYylr/PNXw96/N6+35jTSx+yGoHhXslZr2wB2EBJVB4VOLqNTccPBmCVWk9cb1K3hC3SXeCrzA2ismAxHVWVWBbaiv8muUORycBL04rxILPL2D0gx1QdcNRbPftZz5Yt8hf/Fx14++i0r6t3jj+g5dBv5OW5sPZRc9EgBqcm7wa5y63xPajHZAyMv2W4+OfzEDUPQWY+2kMCku80LPDJbz21EH8UeaOg6faoviqBx5/4zmDcwb9JxvPPvQLjpwJAgAIggwHTrbFR9vvw9VyVwS21uC1YQcx2eMA5qx7uEHuk8zHtxYa16jJwP79+7Fo0SJkZmaisLAQmzdvxuDBgxszJMl5Pb6dwefFE4Kx6dRp3NP5Bk4d9RT3V91wwNUr9X+xaysdoK38OzlQ+tSgy/3lWPJaYL3tiWzpSHYwjmQHGz0eGVqEHcfa46dzAQCArRnhGHR/NsKDr+DgqbbQCw4oueZucE6fzhexO6sdbmhr/x+4dsNFrCQAQNFVL3xz8F48+9DPNrgjshorrTNgjxp1zkBFRQW6dOnyrxMnqGF5KHQAgGuljgb7Y4ZcxaZTp/DBnhyMmF4IFzfjM2linypB1Q0ZDmz3tmWoRHfkZK4fHoj8Ha2UFQAEdA8rQHDrMhzLqT957RB4Be0D/0RaRgejfbZSVODBzrnIOt/GRlET2VajVgYGDBiAAQMGmNy+qqrK4IUQGo3GFmFJlkwmYOycyzh1zB2/5/y9atUPm1ug+JIz/ixyRmh4JUa+XojAu6swb1TbevuJe6YEP2xuYVAtIGoqlnx1P6YO34//zV2PGp0MekGG//dFH/xs5Iv8MVUOctXeOHXR/5Zjs1/Yjd6RF+Eq1+HgyWC89XkfW4dPFuAwgXHNas5ASkoK5syZ09hh2K2kBZcR0rESrw0OM9i/Y31L8c8Xz7qhpNgJC7+8gDYhVSj83XBN7vCoCoS0r8LC8cbLtESNaVifU7g3pBhTPoyD+qonut5diNeGHcIfZe44/qthdUDuXINHup9D6nfd6+1r+WYVPtkZhWDfUox97EeMf/IIFn/5QEPcBt0JTiA0qln96jZ9+nSUlZWJW35+/TPYyXyJ8y8h+hENpgy7G38U/vts6LMnasdTA9re+trO/s+W4NwpV5w76X7LMaLGJneuwf899iOWb1Hh0OkQnC9oia8PdMLun9rhmYd+uaV9TJcLcJXXYOexe+rtr+SaO/KKvXHwVFss3NQbQx44g5aK67a+DSKra1aVARcXF5PeDkXmEJA4/zL+078Mk4eFoSj/9j/fuztVAgBKig0nFLq669Dn8VKsSeG4KTVNTg56ODvpb5kHptPL4FBPDfixXjk4eCoEpRW3f9mL7K/znZ10VomVrI/DBMY1q2SArC9pwWXEPHkVs0eE4ka5A1q0rgYAVFxzhLbSAW1CqhDzZCmO7fbCtatOCI24gf+bXYBfMjyQm234D+SDg0rh6Chg99ctGuNWiADUPloY2LpM/BzQUoN77voDmuuuKLrqiRO/tUHioKOoqnaCusQT3cIKMeC+37B8i+FLXe5qVYaudxdi0ge3zmtSReShhdcNZOe1xo0qZ4T6X0XioCP4+YIf1CVet7SnJoJPExjFZEDiHn/xTwDA29+cN9j/9oQgpG/yQU21DN16X8OTo67A1V2PKwXOOPitEp8v9bulr/7PlODQDiUqNI63HCNqKB2Dr+C98Wni51eePAIA+PZoe8zf0Bez1j6MsY8fw6zn90DhXgX1VU98sP0+bDkUbtDPY71yUFzmUe9TBlVaJzyhOotXBmdA7qRDUakn9v3SFp9939Wm90ZkKzLBlMWTbaS8vBznzp0DAHTr1g3vvPMOYmJi4OPjg+Dg209A02g0UCqV6ItBcJJxcRuyT+VPRTd2CEQ2U1NdieObZ6CsrAwKhcIm16j7rlANmAsnZ9fbn2BETXUlMnbMtGmsjaVRKwPHjx9HTEyM+HnixIkAgISEBKSmpjZSVEREZJf4NIFRjZoM9O3b16S3OhEREZHtcM4AERFJAp8mMI7JABERSYNeqN0sOd9OMRkgIiJp4JwBo5rVCoRERERkfawMEBGRJMhg4ZwBq0XS9DAZICIiaeAKhEZxmICIiEjiWBkgIiJJ4KOFxjEZICIiaeDTBEZxmICIiEjiWBkgIiJJkAkCZBZMArTk3KaOyQAREUmD/q/NkvPtFIcJiIiIJI6VASIikgQOExjHZICIiKSBTxMYxWSAiIikgSsQGsU5A0RERBLHygAREUkCVyA0jskAERFJA4cJjOIwARERkcSxMkBERJIg09dulpxvr5gMEBGRNHCYwCgOExAREUkcKwNERCQNXHTIKCYDREQkCVyO2DgOExAREUkcKwNERCQNnEBoFJMBIiKSBgGAJY8H2m8uwGSAiIikgXMGjOOcASIiIoljZYCIiKRBgIVzBqwWSZPDZICIiKSBEwiN4jABERGRxLEyQERE0qAHILPwfDvFZICIiCSBTxMYx2ECIiIiiWMyQERE0lA3gdCSzQz79+/H448/joCAAMhkMmzZsuUf4QiYOXMm2rRpAzc3N8TGxuK3334zaFNSUoL4+HgoFAp4e3tj5MiRKC8vN2jzyy+/oHfv3nB1dUVQUBAWLlxo9o+GyQAREUlDAycDFRUV6NKlC1asWFHv8YULF2L58uVYtWoVjh49Cg8PD8TFxaGyslJsEx8fj9OnTyM9PR1paWnYv38/xowZIx7XaDTo168fQkJCkJmZiUWLFmH27Nn48MMPzYqVcwaIiIjMoNFoDD67uLjAxcXllnYDBgzAgAED6u1DEAQsXboUb7zxBgYNGgQAWLduHfz8/LBlyxYMHz4c2dnZ2LlzJ3788Uf06NEDAPDuu+/i0Ucfxdtvv42AgACsX78eWq0Wn3zyCeRyOe69915kZWXhnXfeMUgaboeVASIikgYrVQaCgoKgVCrFLSUlxexQcnNzoVarERsbK+5TKpWIjo5GRkYGACAjIwPe3t5iIgAAsbGxcHBwwNGjR8U2ffr0gVwuF9vExcUhJycHV69eNTkeVgaIiEgarPRoYX5+PhQKhbi7vqrA7ajVagCAn5+fwX4/Pz/xmFqthq+vr8FxJycn+Pj4GLQJDQ29pY+6Yy1atDApHiYDREQkCdZ6tFChUBgkA/aAwwREREQNzN/fHwBQVFRksL+oqEg85u/vj+LiYoPjNTU1KCkpMWhTXx83X8MUTAaIiEgaGvhpgn8TGhoKf39/7N69W9yn0Whw9OhRqFQqAIBKpUJpaSkyMzPFNnv27IFer0d0dLTYZv/+/aiurhbbpKeno0OHDiYPEQBMBoiISCr0guWbGcrLy5GVlYWsrCwAtZMGs7KykJeXB5lMhgkTJuDNN9/E1q1bcfLkSbzwwgsICAjA4MGDAQDh4eHo378/Ro8ejWPHjuHQoUNISkrC8OHDERAQAAB49tlnIZfLMXLkSJw+fRobN27EsmXLMHHiRLNi5ZwBIiIiGzh+/DhiYmLEz3Vf0AkJCUhNTcWUKVNQUVGBMWPGoLS0FA888AB27twJV1dX8Zz169cjKSkJDz/8MBwcHDB06FAsX75cPK5UKvHdd98hMTERUVFRaNWqFWbOnGnWY4UAIBOE5rvYskajgVKpRF8MgpPMubHDIbKJ8qeiGzsEIpupqa7E8c0zUFZWZrNJeXXfFbHtXoWTo/kz/+vU6Krw/YVlNo21sbAyQEREEmHpuH+z/d35tjhngIiISOJYGSAiImmw9ImA5juqfltMBoiISBr0Aiwq9Zv5NEFzwmECIiIiiWNlgIiIpEHQ126WnG+nmAwQEZE0cM6AUUwGiIhIGjhnwCjOGSAiIpI4VgaIiEgaOExgFJMBIiKSBgEWJgNWi6TJ4TABERGRxLEyQERE0sBhAqOYDBARkTTo9QAsWCtAb7/rDHCYgIiISOJYGSAiImngMIFRTAaIiEgamAwYxWECIiIiiWNlgIiIpIHLERvFZICIiCRBEPQQLHjzoCXnNnVMBoiISBoEwbLf7jlngIiIiOwVKwNERCQNgoVzBuy4MsBkgIiIpEGvB2QWjPvb8ZwBDhMQERFJHCsDREQkDRwmMIrJABERSYKg10OwYJjAnh8t5DABERGRxLEyQERE0sBhAqOYDBARkTToBUDGZKA+HCYgIiKSOFYGiIhIGgQBgCXrDNhvZYDJABERSYKgFyBYMEwgMBkgIiJq5gQ9LKsM8NFCIiIislOsDBARkSRwmMA4JgNERCQNHCYwqlknA3VZWg2qLVpHgqgpq6mubOwQiGxG99ff74b4rdvS74oaVFsvmCZGJjTjuselS5cQFBTU2GEQEZGF8vPzERgYaJO+KysrERoaCrVabXFf/v7+yM3NhaurqxUiazqadTKg1+tRUFAALy8vyGSyxg5HEjQaDYKCgpCfnw+FQtHY4RBZFf9+NzxBEHDt2jUEBATAwcF2c9orKyuh1Wot7kcul9tdIgA082ECBwcHm2WS9O8UCgX/sSS7xb/fDUupVNr8Gq6urnb5JW4tfLSQiIhI4pgMEBERSRyTATKLi4sLZs2aBRcXl8YOhcjq+PebpKpZTyAkIiIiy7EyQEREJHFMBoiIiCSOyQAREZHEMRkgIiKSOCYDZLIVK1agbdu2cHV1RXR0NI4dO9bYIRFZxf79+/H4448jICAAMpkMW7ZsaeyQiBoUkwEyycaNGzFx4kTMmjULJ06cQJcuXRAXF4fi4uLGDo3IYhUVFejSpQtWrFjR2KEQNQo+WkgmiY6Oxn333Yf33nsPQO17IYKCgjB+/HhMmzatkaMjsh6ZTIbNmzdj8ODBjR0KUYNhZYBuS6vVIjMzE7GxseI+BwcHxMbGIiMjoxEjIyIia2AyQLf1xx9/QKfTwc/Pz2C/n5+fVV4JSkREjYvJABERkcQxGaDbatWqFRwdHVFUVGSwv6ioCP7+/o0UFRERWQuTAbotuVyOqKgo7N69W9yn1+uxe/duqFSqRoyMiIiswamxA6DmYeLEiUhISECPHj3Qs2dPLF26FBUVFRgxYkRjh0ZksfLycpw7d078nJubi6ysLPj4+CA4OLgRIyNqGHy0kEz23nvvYdGiRVCr1ejatSuWL1+O6Ojoxg6LyGJ79+5FTEzMLfsTEhKQmpra8AERNTAmA0RERBLHOQNEREQSx2SAiIhI4pgMEBERSRyTASIiIoljMkBERCRxTAaIiIgkjskAERGRxDEZICIikjgmA0QWevHFFzF48GDxc9++fTFhwoQGj2Pv3r2QyWQoLS012kYmk2HLli0m9zl79mx07drVorguXrwImUyGrKwsi/ohItthMkB26cUXX4RMJoNMJoNcLkdYWBjmzp2Lmpoam1/7m2++wbx580xqa8oXOBGRrfFFRWS3+vfvjzVr1qCqqgrffvstEhMT4ezsjOnTp9/SVqvVQi6XW+W6Pj4+VumHiKihsDJAdsvFxQX+/v4ICQnBuHHjEBsbi61btwL4u7Q/f/58BAQEoEOHDgCA/Px8PP300/D29oaPjw8GDRqEixcvin3qdDpMnDgR3t7eaNmyJaZMmYJ/vt7jn8MEVVVVmDp1KoKCguDi4oKwsDB8/PHHuHjxovhynBYtWkAmk+HFF18EUPuK6JSUFISGhsLNzQ1dunTBV199ZXCdb7/9Fu3bt4ebmxtiYmIM4jTV1KlT0b59e7i7u6Ndu3aYMWMGqqurb2n3wQcfICgoCO7u7nj66adRVlZmcHz16tUIDw+Hq6srOnbsiPfff9/sWIio8TAZIMlwc3ODVqsVP+/evRs5OTlIT09HWloaqqurERcXBy8vLxw4cACHDh2Cp6cn+vfvL563ePFipKam4pNPPsHBgwdRUlKCzZs3/+t1X3jhBXz++edYvnw5srOz8cEHH8DT0xNBQUH4+uuvAQA5OTkoLCzEsmXLAAApKSlYt24dVq1ahdOnTyM5ORnPPfcc9u3bB6A2aRkyZAgef/xxZGVlYdSoUZg2bZrZPxMvLy+kpqbizJkzWLZsGT766CMsWbLEoM25c+ewadMmbNu2DTt37sRPP/2El19+WTy+fv16zJw5E/Pnz0d2djYWLFiAGTNmYO3atWbHQ0SNRCCyQwkJCcKgQYMEQRAEvV4vpKenCy4uLsKkSZPE435+fkJVVZV4zqeffip06NBB0Ov14r6qqirBzc1N2LVrlyAIgtCmTRth4cKF4vHq6mohMDBQvJYgCMKDDz4ovPrqq4IgCEJOTo4AQEhPT683zh9++EEAIFy9elXcV1lZKbi7uwuHDx82aDty5EjhmWeeEQRBEKZPny5EREQYHJ86deotff0TAGHz5s1Gjy9atEiIiooSP8+aNUtwdHQULl26JO7bsWOH4ODgIBQWFgqCIAh33323sGHDBoN+5s2bJ6hUKkEQBCE3N1cAIPz0009Gr0tEjYtzBshupaWlwdPTE9XV1dDr9Xj22Wcxe/Zs8XhkZKTBPIGff/4Z586dg5eXl0E/lZWVOH/+PMrKylBYWIjo6GjxmJOTE3r06HHLUEGdrKwsODo64sEHHzQ57nPnzuH69et45JFHDPZrtVp069YNAJCdnW0QBwCoVCqTr1Fn48aNWL58Oc6fP4/y8nLU1NRAoVAYtAkODsZdd91lcB29Xo+cnBx4eXnh/PnzGDlyJEaPHi22qampgVKpNDseImocTAbIbsXExGDlypWQy+UICAiAk5PhX3cPDw+Dz+Xl5YiKisL69etv6at169Z3FIObm5vZ55SXlwMAtm/fbvAlDNTOg7CWjIwMxMfHY86cOYiLi4NSqcQXX3yBxYsXmx3rRx99dEty4ujoaLVYici2mAyQ3fLw8EBYWJjJ7bt3746NGzfC19f3lt+O67Rp0wZHjx5Fnz59ANT+BpyZmYnu3bvX2z4yMhJ6vR779u1DbGzsLcfrKhM6nU7cFxERARcXF+Tl5RmtKISHh4uTIescOXLk9jd5k8OHDyMkJASvv/66uO/333+/pV1eXh4KCgoQEBAgXsfBwQEdOnSAn58fAgICcOHCBcTHx5t1fSJqOjiBkOgv8fHxaNWqFQYNGoQDBw4gNzcXe/fuxSuvvIJLly4BAF599VW89dZb2LJlC86ePYuXX375X9cIaNu2LRISEvDSSy9hy5YtYp+bNm0CAISEhEAmkyEtLQ1XrlxBeXk5vLy8MGnSJCQnJ2Pt2rU4f/48Tpw4gXfffVeclDd27Fj89ttvmDx5MnJycrBhwwakpqaadb/33HMP8vLy8MUXX+D8+fNYvnx5vZMhXV1dkZCQgJ9//hkHDhzAK6+8gqeffhr+/v4AgDlz5iAlJQXLly/Hr7/+ipMnT2LNmjV45513zIqHiBoPkwGiv7i7u2P//v0IDg7GkCFDEB4ejpEjR6KyslKsFLz22mt4/vnnkZCQAJVKBS8vLzz55JP/2u/KlSsxbNgwvPzyy+jYsSNGjx6NiooKAMBdd92FOXPmYNq0afDz80NSUhIAYN68eZgxYwZSUlIQHh6O/v37Y/v27QgNDQVQO47/9ddfY8uWLejSpQtWrVqFBQsWmHW/TzzxBJKTk5GUlISuXbvi8OHDmDFjxi3twsLCMGTIEDz66KPo168fOnfubPDo4KhRo7B69WqsWbMGkZGRePDBB5GamirGSkRNn0wwNvOJiIiIJIGVASIiIoljMkBERCRxTAaIiIgkjskAERGRxDEZICIikjgmA0RERBLHZICIiEjimAwQERFJHJMBIiIiiWMyQEREJHFMBoiIiCTu/wdhpxx7eVY2LQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: [400, 200, 16]\n",
      "Epoch 0, Loss: 0.6176791949984953\n",
      "Epoch 1, Loss: 0.5415282986827732\n",
      "Epoch 2, Loss: 0.5336586734683243\n",
      "Epoch 3, Loss: 0.5355373077171365\n",
      "Epoch 4, Loss: 0.5349282668423407\n",
      "Epoch 5, Loss: 0.538160602763756\n",
      "Epoch 6, Loss: 0.5289279612683758\n",
      "Epoch 7, Loss: 0.535454624092456\n",
      "Epoch 8, Loss: 0.5304220610672665\n",
      "Epoch 9, Loss: 0.5299766963904666\n",
      "Architecture: [400, 200, 32]\n",
      "Epoch 0, Loss: 0.6038210287536543\n",
      "Epoch 1, Loss: 0.5451834552066842\n",
      "Epoch 2, Loss: 0.5360191458279324\n",
      "Epoch 3, Loss: 0.5392557662172416\n",
      "Epoch 4, Loss: 0.534663853571587\n",
      "Epoch 5, Loss: 0.531411486188161\n",
      "Epoch 6, Loss: 0.5323531793564865\n",
      "Epoch 7, Loss: 0.5356707364013514\n",
      "Epoch 8, Loss: 0.5302860119293645\n",
      "Epoch 9, Loss: 0.5361622657972512\n",
      "Architecture: [500, 200, 16]\n",
      "Epoch 0, Loss: 0.5989350068200495\n",
      "Epoch 1, Loss: 0.5480544385836297\n",
      "Epoch 2, Loss: 0.5424567430289751\n",
      "Epoch 3, Loss: 0.5363637860288325\n",
      "Epoch 4, Loss: 0.5356828591872737\n",
      "Epoch 5, Loss: 0.536745577743373\n",
      "Epoch 6, Loss: 0.5399520778164422\n",
      "Epoch 7, Loss: 0.5313663467303994\n",
      "Epoch 8, Loss: 0.5356627291010827\n",
      "Epoch 9, Loss: 0.530924648353734\n",
      "Architecture: [500, 200, 32]\n",
      "Epoch 0, Loss: 0.592678164698414\n",
      "Epoch 1, Loss: 0.5479760646205587\n",
      "Epoch 2, Loss: 0.5391252658416316\n",
      "Epoch 3, Loss: 0.5385614059020564\n",
      "Epoch 4, Loss: 0.5367355653920125\n",
      "Epoch 5, Loss: 0.5367987503096\n",
      "Epoch 6, Loss: 0.5334534331695321\n",
      "Epoch 7, Loss: 0.538361195436458\n",
      "Epoch 8, Loss: 0.5368435677793837\n",
      "Epoch 9, Loss: 0.5312410488571089\n",
      "Architecture: [600, 200, 16]\n",
      "Epoch 0, Loss: 0.6231693978776637\n",
      "Epoch 1, Loss: 0.5579463977789142\n",
      "Epoch 2, Loss: 0.5529795851289612\n",
      "Epoch 3, Loss: 0.5490585181516471\n",
      "Epoch 4, Loss: 0.5420227871113217\n",
      "Epoch 5, Loss: 0.5402340302147817\n",
      "Epoch 6, Loss: 0.5410559595245676\n",
      "Epoch 7, Loss: 0.544125275820801\n",
      "Epoch 8, Loss: 0.5372172514187921\n",
      "Epoch 9, Loss: 0.534747892433835\n",
      "Architecture: [600, 200, 32]\n",
      "Epoch 0, Loss: 0.5853576528042862\n",
      "Epoch 1, Loss: 0.5528767320913138\n",
      "Epoch 2, Loss: 0.5555805459464949\n",
      "Epoch 3, Loss: 0.5520730457969547\n",
      "Epoch 4, Loss: 0.541073239648465\n",
      "Epoch 5, Loss: 0.5407328378293932\n",
      "Epoch 6, Loss: 0.5394224650466565\n",
      "Epoch 7, Loss: 0.53626421984938\n",
      "Epoch 8, Loss: 0.5473887926524448\n",
      "Epoch 9, Loss: 0.5393490954158232\n"
     ]
    }
   ],
   "source": [
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, arch):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, arch[0])\n",
    "\n",
    "        self.fc1 = Linear(arch[0], arch[1])\n",
    "        self.fc2 = Linear(arch[1], arch[2])\n",
    "\n",
    "        self.classifier = Linear(arch[2], num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "architectures = [[400, 200, 16], [400, 200, 32], [500, 200, 16], [500, 200, 32], [600, 200, 16], [600, 200, 32]]\n",
    "\n",
    "best_f1 = 0\n",
    "best_arch = None\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"Architecture: {arch}\")\n",
    "    model = GCNModel(num_features=NUM_FEATURES, num_classes=NUM_CLASSES, arch=arch).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0\n",
    "        for data in graphs_data:  # Remplacez par vos données d'entraînement\n",
    "            data = data.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(graphs_data)}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data in graphs_data:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        pred = pred[data.test_mask]\n",
    "        true_labels = data.y[data.test_mask]\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(true_labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_arch = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38186932064041545"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouvelles Données Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recup des ids\n",
    "test_transcript_ids = []\n",
    "\n",
    "test_transcripts = path_to_test.glob('*.json')\n",
    "for test_transcript in test_transcripts:\n",
    "    test_transcript_ids.append(test_transcript.name[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 20/20 [00:01<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:01<00:00,  9.76it/s]\n",
      "Batches: 100%|██████████| 20/20 [00:02<00:00,  9.17it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:02<00:00, 10.41it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:02<00:00, 10.87it/s]\n",
      "Batches: 100%|██████████| 20/20 [00:01<00:00, 12.61it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:02<00:00,  9.69it/s]\n",
      "Batches: 100%|██████████| 15/15 [00:01<00:00,  8.90it/s]\n",
      "Batches: 100%|██████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "Batches: 100%|██████████| 19/19 [00:01<00:00, 10.99it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:02<00:00, 10.72it/s]\n",
      "Batches: 100%|██████████| 20/20 [00:02<00:00,  9.92it/s]\n",
      "Batches: 100%|██████████| 14/14 [00:01<00:00, 10.89it/s]\n",
      "Batches: 100%|██████████| 21/21 [00:01<00:00, 11.68it/s]\n",
      "Batches: 100%|██████████| 15/15 [00:01<00:00, 11.33it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:02<00:00, 10.20it/s]\n",
      "Batches: 100%|██████████| 35/35 [00:03<00:00, 10.28it/s]\n",
      "Batches: 100%|██████████| 14/14 [00:01<00:00,  8.96it/s]\n",
      "Batches: 100%|██████████| 27/27 [00:02<00:00,  9.75it/s]\n",
      "Batches: 100%|██████████| 30/30 [00:03<00:00,  9.56it/s]\n",
      "Batches: 100%|██████████| 29/29 [00:02<00:00, 10.94it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:02<00:00, 10.96it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:01<00:00, 11.04it/s]\n",
      "Batches: 100%|██████████| 52/52 [00:04<00:00, 11.62it/s]\n",
      "Batches: 100%|██████████| 30/30 [00:02<00:00, 10.61it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:01<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:01<00:00,  9.31it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:02<00:00,  9.35it/s]\n",
      "Batches: 100%|██████████| 37/37 [00:03<00:00, 11.45it/s]\n",
      "Batches: 100%|██████████| 40/40 [00:03<00:00, 12.01it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:02<00:00, 10.25it/s]\n",
      "Batches: 100%|██████████| 32/32 [00:02<00:00, 12.24it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:03<00:00, 12.42it/s]\n",
      "Batches: 100%|██████████| 9/9 [00:01<00:00,  8.80it/s]\n",
      "Batches: 100%|██████████| 10/10 [00:01<00:00,  8.28it/s]\n",
      "Batches: 100%|██████████| 34/34 [00:02<00:00, 11.54it/s]\n",
      "Batches: 100%|██████████| 38/38 [00:03<00:00, 11.87it/s]\n",
      "Batches: 100%|██████████| 37/37 [00:03<00:00, 11.74it/s]\n",
      "Batches: 100%|██████████| 24/24 [00:02<00:00,  9.76it/s]\n",
      "Batches: 100%|██████████| 46/46 [00:03<00:00, 11.90it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = [create_data_object(dialogue, path_to_data =path_to_test) for dialogue in test_transcript_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_gcn = {}\n",
    "for i in range(len(test_transcript_ids)):\n",
    "    transcription_id = test_transcript_ids[i]\n",
    "    data = test_data[i]\n",
    "    data = data.to(device)\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    pred = pred.cpu().numpy()\n",
    "    test_labels_gcn[transcription_id] = pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = set()\n",
    "for keys in test_labels_gcn.keys():\n",
    "    uniques.update(test_labels_gcn[keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.fc = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #print(x.shape)  # Doit être de la forme [num_nodes, num_features]\n",
    "        #print(edge_index.shape)  # Doit être de la forme [2, num_edges]\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_max_pool(x, data.batch, size=data.num_nodes)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
